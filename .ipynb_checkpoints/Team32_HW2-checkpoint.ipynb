{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "#mnist = input_data.read_data_sets(\"./\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "###### Do not modify here ######\n",
    "\n",
    "#use variance_scaling_initializer to initialize variable\n",
    "def init_var(name, shape):\n",
    "    return tf.get_variable(name, shape, dtype=tf.float32, \n",
    "                           initializer=variance_scaling_initializer(factor=1.0))\n",
    "\n",
    "num_class = 5\n",
    "batch_size = 128\n",
    "dropout_keep_prob = 1 #1 for disable dropout\n",
    "hidden_neuron_size = 128\n",
    "\n",
    "#convert label to one-hot encoding\n",
    "def label_to_onehot(label):\n",
    "    result = np.empty([label.shape[0], num_class])\n",
    "    for i in range(label.shape[0]):\n",
    "        enc = np.zeros(5)\n",
    "        enc[label[i]] = 1.0\n",
    "        result[i] = enc\n",
    "    return result\n",
    "\n",
    "#do one-hot encoding\n",
    "y_train1 = label_to_onehot(y_train1)\n",
    "y_valid1 = label_to_onehot(y_valid1)\n",
    "y_test1 = label_to_onehot(y_test1)\n",
    "\n",
    "#return one batch a time\n",
    "def batch_generator(dataX, dataY):\n",
    "    i = 0\n",
    "    while (i + 1) * batch_size < len(dataX):\n",
    "        i_from = i * batch_size\n",
    "        i_to = (i + 1) * batch_size\n",
    "        yield dataX[i_from : i_to], dataY[i_from : i_to]\n",
    "        i += 1\n",
    "    i_from = i * batch_size\n",
    "    i_to = (i + 1) * batch_size\n",
    "    yield dataX[i_from : i_to], dataY[i_from : i_to]\n",
    "\n",
    "#define neuron network\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='x') #28*28\n",
    "y = tf.placeholder(tf.float32, [None, num_class], name='y')\n",
    "#define dropout rate\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = init_var(\"W1\", [784, hidden_neuron_size])\n",
    "b1 = init_var(\"b1\", [hidden_neuron_size])\n",
    "out1 = tf.nn.elu(tf.matmul(x, W1) + b1, name='out1')\n",
    "#out1_drop = tf.nn.dropout(out1, keep_prob)\n",
    "\n",
    "W2 = init_var(\"W2\", [hidden_neuron_size, hidden_neuron_size])\n",
    "b2 = init_var(\"b2\", [hidden_neuron_size])\n",
    "out2 = tf.nn.elu(tf.matmul(out1, W2) + b2, name='out2')\n",
    "#out2_drop = tf.nn.dropout(out2, keep_prob)\n",
    "\n",
    "W3 = init_var(\"W3\", [hidden_neuron_size, hidden_neuron_size])\n",
    "b3 = init_var(\"b3\", [hidden_neuron_size])\n",
    "out3 = tf.nn.elu(tf.matmul(out2, W3) + b3, name='out3')\n",
    "#out3_drop = tf.nn.dropout(out3, keep_prob)\n",
    "\n",
    "W4 = init_var(\"W4\", [hidden_neuron_size, hidden_neuron_size])\n",
    "b4 = init_var(\"b4\", [hidden_neuron_size])\n",
    "out4 = tf.nn.elu(tf.matmul(out3, W4) + b4, name='out4')\n",
    "#out4_drop = tf.nn.dropout(out4, keep_prob)\n",
    "\n",
    "W5 = init_var(\"W5\", [hidden_neuron_size, hidden_neuron_size])\n",
    "b5 = init_var(\"b5\", [hidden_neuron_size])\n",
    "out5 = tf.nn.elu(tf.matmul(out4, W5) + b5, name='out5')\n",
    "\n",
    "Wo = init_var(\"Wo\", [hidden_neuron_size, num_class])\n",
    "bo = init_var(\"bo\", [num_class])\n",
    "logits = tf.add(tf.matmul(out5, Wo), bo)\n",
    "y_prob = tf.nn.softmax(logits, name='y_prob')\n",
    "\n",
    "#use cross-entropy loss\n",
    "xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = logits), name='loss')\n",
    "\n",
    "#use Adam to spped up training\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(xent, name='training_op')\n",
    "\n",
    "#implement early stop mechanism\n",
    "class early_stop():\n",
    "    def __init__(self, stop_margin, max_epochs, threshold):\n",
    "        self.val_not_better_counter = 0 #count epoch after best validation error, >stop_margin then stop\n",
    "        self.stop_margin = stop_margin\n",
    "        self.best_val_acc = 0\n",
    "        self.epoch_counter = max_epochs\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def monitor(self, val_acc):        \n",
    "        if(val_acc > self.best_val_acc):\n",
    "            #record best validation accuracy so far, and reset counter\n",
    "            self.best_val_acc = val_acc\n",
    "            self.val_not_better_counter = 0\n",
    "            \n",
    "        val_err = 1 - val_acc\n",
    "        best_val_err = 1 - self.best_val_acc\n",
    "        \n",
    "        if(val_err / best_val_err > self.threshold):\n",
    "            #if current validation error divide by best validation error so far greater than\n",
    "            #threshold, counter++\n",
    "            self.val_not_better_counter += 1\n",
    "        \n",
    "        self.epoch_counter -= 1\n",
    "            \n",
    "    def continue_training(self):\n",
    "        return self.val_not_better_counter < self.stop_margin and self.epoch_counter > 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "for kp in [1.0]:\n",
    "    #dropout_keep_prob = kp\n",
    "    with tf.Session() as sess:\n",
    "        #print(\"Dropout rate: \", 1 - dropout_keep_prob, \"training start.\")\n",
    "        els = early_stop(stop_margin = 5,\n",
    "                        max_epochs = 200,\n",
    "                        threshold = 1.5)\n",
    "        sess.run(tf.global_variables_initializer()) #init\n",
    "        \n",
    "        while(els.continue_training()):\n",
    "            #train\n",
    "            sum_loss = 0\n",
    "            for batchX, batchY in batch_generator(X_train1, y_train1):\n",
    "                _, loss = sess.run([train_step, xent], feed_dict={x: batchX, y: batchY})\n",
    "                sum_loss += loss\n",
    "            avg_loss = sum_loss / (len(X_train1) // batch_size + 1)\n",
    "\n",
    "            #validate\n",
    "            y_pre = sess.run(y_prob, feed_dict={x: X_valid1})\n",
    "            correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_valid1, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "            val_acc = sess.run(accuracy, feed_dict={x: X_valid1})\n",
    "            els.monitor(val_acc)\n",
    "\n",
    "        #test\n",
    "        y_t = sess.run(y_prob, feed_dict={x: X_test1})\n",
    "        y_pre_max, y_real_max = sess.run(tf.argmax(y_t, 1)), sess.run(tf.argmax(y_test1, 1))\n",
    "\n",
    "        tp = np.zeros(num_class) #store \"True Positive\" count for every label\n",
    "        tn = np.zeros(num_class) #store \"True Negtive\" count for every label\n",
    "        fp = np.zeros(num_class) #store \"False Positive\" count for every label\n",
    "        fn = np.zeros(num_class) #store \"False Negtive\" count for every label\n",
    "\n",
    "        for i in range(y_pre_max.shape[0]):\n",
    "            if(y_pre_max[i] == y_real_max[i]):\n",
    "                #if predict == truth, that label count as \"True Positive\"\n",
    "                #and others count as \"True Negtive\"\n",
    "                tp[y_pre_max[i]] += 1\n",
    "                tn[[x for x in range(num_class) if x != y_pre_max[i]]] += 1\n",
    "            else:\n",
    "                #if predict != truth, predict label count as \"False Positive\"\n",
    "                #and trhth label count as \"False Negtive\"\n",
    "                #and others count as \"True Negtive\"\n",
    "                fp[y_pre_max[i]] += 1\n",
    "                fn[y_real_max[i]] += 1\n",
    "                tn[[x for x in range(num_class) if x != y_pre_max[i] and x != y_real_max[i]]] += 1        \n",
    "\n",
    "        for i in range(num_class):\n",
    "            #calculate Precision and Recall for every lebel\n",
    "            prec = tp[i] / (tp[i] + fp[i])\n",
    "            recall = tp[i] / (tp[i] + fn[i])\n",
    "            print(\"Label {0}, Precision: {1:.2f}%, Recall: {2:.2f}%\".format(i, prec * 100, recall * 100))\n",
    "        #calculate overall accuracy\n",
    "        acc = (np.sum(tp) + np.sum(tn)) / (np.sum(tp) + np.sum(tn) + np.sum(fp) + np.sum(fn))\n",
    "        print(\"Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "        #save training\n",
    "        saver.save(sess, './Team32_HW2.ckpt')\n",
    "        print('saved!')\n",
    "    \n",
    "\"\"\"\n",
    "Early stopping機制:\n",
    "計算Generalization error(目前的validation error/目前最好的validation error)\n",
    "若大於threshold則將counter+1\n",
    "counter大於設定數值則停止訓練\n",
    "最多不訓練超過設定的epoch數值\n",
    "\n",
    "\n",
    "Dropout result:\n",
    "Dropout rate: 0(100% keep)\n",
    "Label 0, Precision: 99.49%, Recall: 99.49%\n",
    "Label 1, Precision: 99.91%, Recall: 99.74%\n",
    "Label 2, Precision: 98.74%, Recall: 98.74%\n",
    "Label 3, Precision: 99.11%, Recall: 99.50%\n",
    "Label 4, Precision: 99.49%, Recall: 99.29%\n",
    "Accuracy: 99.74%\n",
    "\n",
    "Dropout rate: 0.1(90% keep)\n",
    "Label 0, Precision: 98.89%, Recall: 99.69%\n",
    "Label 1, Precision: 99.30%, Recall: 99.91%\n",
    "Label 2, Precision: 98.82%, Recall: 97.19%\n",
    "Label 3, Precision: 98.82%, Recall: 99.50%\n",
    "Label 4, Precision: 99.49%, Recall: 98.98%\n",
    "Accuracy: 99.63%\n",
    "\n",
    "Dropout rate: 0.2(80% keep)\n",
    "Label 0, Precision: 99.59%, Recall: 99.69%\n",
    "Label 1, Precision: 99.56%, Recall: 99.91%\n",
    "Label 2, Precision: 98.64%, Recall: 98.55%\n",
    "Label 3, Precision: 99.41%, Recall: 99.31%\n",
    "Label 4, Precision: 99.49%, Recall: 99.19%\n",
    "Accuracy: 99.74%\n",
    "\n",
    "Dropout rate: 0.3(70% keep)\n",
    "Label 0, Precision: 99.39%, Recall: 99.49%\n",
    "Label 1, Precision: 99.47%, Recall: 99.91%\n",
    "Label 2, Precision: 99.21%, Recall: 97.77%\n",
    "Label 3, Precision: 98.73%, Recall: 99.70%\n",
    "Label 4, Precision: 99.49%, Recall: 99.39%\n",
    "Accuracy: 99.70%\n",
    "\n",
    "Dropout rate: 0.4(60% keep)\n",
    "Label 0, Precision: 99.80%, Recall: 99.49%\n",
    "Label 1, Precision: 99.13%, Recall: 99.91%\n",
    "Label 2, Precision: 98.83%, Recall: 98.45%\n",
    "Label 3, Precision: 99.31%, Recall: 99.60%\n",
    "Label 4, Precision: 99.49%, Recall: 98.98%\n",
    "Accuracy: 99.72%\n",
    "\n",
    "Dropout rate: 0.5(50% keep)\n",
    "Label 0, Precision: 99.49%, Recall: 99.39%\n",
    "Label 1, Precision: 99.82%, Recall: 99.82%\n",
    "Label 2, Precision: 98.93%, Recall: 98.74%\n",
    "Label 3, Precision: 99.21%, Recall: 99.41%\n",
    "Label 4, Precision: 99.19%, Recall: 99.29%\n",
    "Accuracy: 99.74%\n",
    "\n",
    "Dropout rate: 0.6(40% keep)\n",
    "Label 0, Precision: 99.69%, Recall: 99.49%\n",
    "Label 1, Precision: 99.30%, Recall: 99.74%\n",
    "Label 2, Precision: 98.73%, Recall: 97.87%\n",
    "Label 3, Precision: 98.72%, Recall: 99.11%\n",
    "Label 4, Precision: 99.09%, Recall: 99.29%\n",
    "\n",
    "Accuracy: 99.64%\n",
    "Dropout rate: 0.7(30% keep)\n",
    "Label 0, Precision: 99.59%, Recall: 99.29%\n",
    "Label 1, Precision: 99.30%, Recall: 99.38%\n",
    "Label 2, Precision: 97.76%, Recall: 97.19%\n",
    "Label 3, Precision: 98.13%, Recall: 98.71%\n",
    "Label 4, Precision: 98.58%, Recall: 98.78%\n",
    "\n",
    "Accuracy: 99.47%\n",
    "Dropout rate: 0.8(20% keep)\n",
    "Label 0, Precision: 98.98%, Recall: 99.29%\n",
    "Label 1, Precision: 99.38%, Recall: 99.21%\n",
    "Label 2, Precision: 96.48%, Recall: 95.74%\n",
    "Label 3, Precision: 97.33%, Recall: 97.62%\n",
    "Label 4, Precision: 98.07%, Recall: 98.47%\n",
    "\n",
    "Accuracy: 99.23%\n",
    "Dropout rate: 0.9(10% keep)\n",
    "Label 0, Precision: 97.88%, Recall: 98.98%\n",
    "Label 1, Precision: 99.02%, Recall: 98.33%\n",
    "Label 2, Precision: 96.39%, Recall: 93.12%\n",
    "Label 3, Precision: 95.87%, Recall: 96.53%\n",
    "Label 4, Precision: 96.23%, Recall: 98.68%\n",
    "Accuracy: 98.85%\n",
    "\n",
    "結論:dropout對於accuracy有一定的影響，在這個例子中因為訓練的任務比較簡單所以提升不明顯，但是可以看到dorpout rate在很高的時候\n",
    "accuracy反而下降了，這可能是因為一次dorpout的neuron太多，不能勝任這個任務的緣故\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
