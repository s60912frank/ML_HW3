{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:   1, Loss: 1.79848647, Accuracy: 26.667%\n",
      "Epoch:   2, Loss: 1.65778708, Accuracy: 24.667%\n",
      "Epoch:   3, Loss: 1.51020586, Accuracy: 52.667%\n",
      "Epoch:   4, Loss: 1.43087649, Accuracy: 48.667%\n",
      "Epoch:   5, Loss: 1.36191952, Accuracy: 62.000%\n",
      "Epoch:   6, Loss: 1.30241823, Accuracy: 63.333%\n",
      "Epoch:   7, Loss: 1.25005722, Accuracy: 65.333%\n",
      "Epoch:   8, Loss: 1.20372045, Accuracy: 66.000%\n",
      "Epoch:   9, Loss: 1.16237962, Accuracy: 66.667%\n",
      "Epoch:  10, Loss: 1.12527347, Accuracy: 66.667%\n",
      "Epoch:  11, Loss: 1.09177649, Accuracy: 67.333%\n",
      "Epoch:  12, Loss: 1.06138396, Accuracy: 67.333%\n",
      "Epoch:  13, Loss: 1.03367960, Accuracy: 68.000%\n",
      "Epoch:  14, Loss: 1.00831890, Accuracy: 68.000%\n",
      "Epoch:  15, Loss: 0.98501265, Accuracy: 68.667%\n",
      "Epoch:  16, Loss: 0.96351725, Accuracy: 69.333%\n",
      "Epoch:  17, Loss: 0.94362587, Accuracy: 69.333%\n",
      "Epoch:  18, Loss: 0.92516184, Accuracy: 69.333%\n",
      "Epoch:  19, Loss: 0.90797287, Accuracy: 68.667%\n",
      "Epoch:  20, Loss: 0.89192784, Accuracy: 69.333%\n",
      "Epoch:  21, Loss: 0.87691259, Accuracy: 69.333%\n",
      "Epoch:  22, Loss: 0.86282778, Accuracy: 69.333%\n",
      "Epoch:  23, Loss: 0.84958589, Accuracy: 69.333%\n",
      "Epoch:  24, Loss: 0.83711046, Accuracy: 69.333%\n",
      "Epoch:  25, Loss: 0.82533360, Accuracy: 69.333%\n",
      "Epoch:  26, Loss: 0.81419545, Accuracy: 70.000%\n",
      "Epoch:  27, Loss: 0.80364251, Accuracy: 70.000%\n",
      "Epoch:  28, Loss: 0.79362720, Accuracy: 70.000%\n",
      "Epoch:  29, Loss: 0.78410691, Accuracy: 70.667%\n",
      "Epoch:  30, Loss: 0.77504343, Accuracy: 70.667%\n",
      "Epoch:  31, Loss: 0.76640242, Accuracy: 70.667%\n",
      "Epoch:  32, Loss: 0.75815272, Accuracy: 70.667%\n",
      "Epoch:  33, Loss: 0.75026637, Accuracy: 70.667%\n",
      "Epoch:  34, Loss: 0.74271786, Accuracy: 70.667%\n",
      "Epoch:  35, Loss: 0.73548436, Accuracy: 70.667%\n",
      "Epoch:  36, Loss: 0.72854441, Accuracy: 70.667%\n",
      "Epoch:  37, Loss: 0.72187918, Accuracy: 70.667%\n",
      "Epoch:  38, Loss: 0.71547097, Accuracy: 70.667%\n",
      "Epoch:  39, Loss: 0.70930368, Accuracy: 71.333%\n",
      "Epoch:  40, Loss: 0.70336270, Accuracy: 71.333%\n",
      "Epoch:  41, Loss: 0.69763440, Accuracy: 72.000%\n",
      "Epoch:  42, Loss: 0.69210637, Accuracy: 72.000%\n",
      "Epoch:  43, Loss: 0.68676698, Accuracy: 72.000%\n",
      "Epoch:  44, Loss: 0.68160576, Accuracy: 72.000%\n",
      "Epoch:  45, Loss: 0.67661273, Accuracy: 72.667%\n",
      "Epoch:  46, Loss: 0.67177892, Accuracy: 72.667%\n",
      "Epoch:  47, Loss: 0.66709590, Accuracy: 73.333%\n",
      "Epoch:  48, Loss: 0.66255563, Accuracy: 73.333%\n",
      "Epoch:  49, Loss: 0.65815103, Accuracy: 73.333%\n",
      "Epoch:  50, Loss: 0.65387517, Accuracy: 73.333%\n",
      "Epoch:  51, Loss: 0.64972174, Accuracy: 73.333%\n",
      "Epoch:  52, Loss: 0.64568472, Accuracy: 73.333%\n",
      "Epoch:  53, Loss: 0.64175874, Accuracy: 73.333%\n",
      "Epoch:  54, Loss: 0.63793856, Accuracy: 73.333%\n",
      "Epoch:  55, Loss: 0.63421929, Accuracy: 73.333%\n",
      "Epoch:  56, Loss: 0.63059646, Accuracy: 73.333%\n",
      "Epoch:  57, Loss: 0.62706578, Accuracy: 73.333%\n",
      "Epoch:  58, Loss: 0.62362319, Accuracy: 73.333%\n",
      "Epoch:  59, Loss: 0.62026483, Accuracy: 73.333%\n",
      "Epoch:  60, Loss: 0.61698735, Accuracy: 74.000%\n",
      "Epoch:  61, Loss: 0.61378723, Accuracy: 74.000%\n",
      "Epoch:  62, Loss: 0.61066133, Accuracy: 74.000%\n",
      "Epoch:  63, Loss: 0.60760677, Accuracy: 74.000%\n",
      "Epoch:  64, Loss: 0.60462064, Accuracy: 74.000%\n",
      "Epoch:  65, Loss: 0.60170031, Accuracy: 74.000%\n",
      "Epoch:  66, Loss: 0.59884322, Accuracy: 74.000%\n",
      "Epoch:  67, Loss: 0.59604698, Accuracy: 74.000%\n",
      "Epoch:  68, Loss: 0.59330934, Accuracy: 74.000%\n",
      "Epoch:  69, Loss: 0.59062815, Accuracy: 74.000%\n",
      "Epoch:  70, Loss: 0.58800137, Accuracy: 74.000%\n",
      "Epoch:  71, Loss: 0.58542717, Accuracy: 74.000%\n",
      "Epoch:  72, Loss: 0.58290344, Accuracy: 74.000%\n",
      "Epoch:  73, Loss: 0.58042866, Accuracy: 74.000%\n",
      "Epoch:  74, Loss: 0.57800102, Accuracy: 74.000%\n",
      "Epoch:  75, Loss: 0.57561904, Accuracy: 74.667%\n",
      "Epoch:  76, Loss: 0.57328123, Accuracy: 74.667%\n",
      "Epoch:  77, Loss: 0.57098597, Accuracy: 74.667%\n",
      "Epoch:  78, Loss: 0.56873208, Accuracy: 74.667%\n",
      "Epoch:  79, Loss: 0.56651807, Accuracy: 74.667%\n",
      "Epoch:  80, Loss: 0.56434274, Accuracy: 74.667%\n",
      "Epoch:  81, Loss: 0.56220502, Accuracy: 74.667%\n",
      "Epoch:  82, Loss: 0.56010354, Accuracy: 74.667%\n",
      "Epoch:  83, Loss: 0.55803734, Accuracy: 74.667%\n",
      "Epoch:  84, Loss: 0.55600530, Accuracy: 74.667%\n",
      "Epoch:  85, Loss: 0.55400652, Accuracy: 74.667%\n",
      "Epoch:  86, Loss: 0.55203986, Accuracy: 74.667%\n",
      "Epoch:  87, Loss: 0.55010456, Accuracy: 74.667%\n",
      "Epoch:  88, Loss: 0.54819959, Accuracy: 74.667%\n",
      "Epoch:  89, Loss: 0.54632413, Accuracy: 74.667%\n",
      "Epoch:  90, Loss: 0.54447752, Accuracy: 74.667%\n",
      "Epoch:  91, Loss: 0.54265875, Accuracy: 74.667%\n",
      "Epoch:  92, Loss: 0.54086715, Accuracy: 74.667%\n",
      "Epoch:  93, Loss: 0.53910190, Accuracy: 74.667%\n",
      "Epoch:  94, Loss: 0.53736258, Accuracy: 74.667%\n",
      "Epoch:  95, Loss: 0.53564823, Accuracy: 74.667%\n",
      "Model saved!\n",
      "Test accuracy: 77.227%\n",
      "Total training time: 686.030ms\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5\n",
    "\n",
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n",
    "\n",
    "'''\n",
    "Homework 3-1\n",
    "'''\n",
    "\n",
    "#Early stop mechanism\n",
    "class early_stop():\n",
    "    def __init__(self, stop_margin, max_epochs):\n",
    "        self.val_not_better_counter = 0 # Count epoch after best validation error > stop_margin then stop\n",
    "        self.stop_margin = stop_margin # Count epochs if no progress\n",
    "        self.epoch_counter = 0 # Record epoch count\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.best_acc = 0 # Record best accuracy so far\n",
    "        \n",
    "    def monitor(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc # If accuracy better than before, record it as best result so far\n",
    "            self.val_not_better_counter = 0 # Reset counter\n",
    "        else:\n",
    "            self.val_not_better_counter += 1 # If no progress, counter++\n",
    "        self.epoch_counter += 1 # Count epochs\n",
    "            \n",
    "    def continue_training(self):\n",
    "        # Continue training if no progress epochs < stop_margin epochs and current epoch < max_epochs\n",
    "        return self.val_not_better_counter < self.stop_margin and self.epoch_counter < self.max_epochs\n",
    "    \n",
    "num_class = 5\n",
    "\n",
    "# Convert label to one-hot encoding matrix\n",
    "def label_to_onehot(label):\n",
    "    result = np.empty([label.shape[0], num_class])\n",
    "    for i in range(label.shape[0]):\n",
    "        enc = np.zeros(5)\n",
    "        enc[label[i]] = 1.0\n",
    "        result[i] = enc\n",
    "    return result\n",
    "\n",
    "y_train2 = label_to_onehot(y_train2)\n",
    "y_valid2 = label_to_onehot(y_valid2)\n",
    "y_test2 = label_to_onehot(y_test2)\n",
    "\n",
    "# Load model from homework2\n",
    "restore_saver = tf.train.import_meta_graph('./Team32_HW2.ckpt.meta')\n",
    "\n",
    "# Get necessary variables/tensors\n",
    "x = tf.get_default_graph().get_tensor_by_name('x:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "loss_func = tf.get_default_graph().get_tensor_by_name('loss:0')\n",
    "y_prob = tf.get_default_graph().get_tensor_by_name('y_prob:0')\n",
    "output_layer_vars_W = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Wo')\n",
    "output_layer_vars_b = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='bo')\n",
    "# Train only on selected variables\n",
    "training_op = tf.train.GradientDescentOptimizer(1).minimize(loss_func, var_list=[output_layer_vars_W, output_layer_vars_b])\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #init\n",
    "    els = early_stop(stop_margin = 20,\n",
    "                    max_epochs = 1000)\n",
    "    \n",
    "    # Record total training time\n",
    "    totalTime = 0        \n",
    "    while(els.continue_training()):\n",
    "        tStart = time()\n",
    "        _, loss = sess.run([training_op, loss_func], feed_dict={x: X_train2, y: y_train2})\n",
    "        totalTime += time() - tStart\n",
    "        loss = np.mean(loss)\n",
    "        \n",
    "        #validate\n",
    "        y_pre = sess.run(y_prob, feed_dict={x: X_valid2})\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_valid2, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        val_acc = sess.run(accuracy, feed_dict={x: X_valid2})\n",
    "        \n",
    "        els.monitor(val_acc)\n",
    "        print('Epoch: {:3}, Loss: {:.8f}, Accuracy: {:.3f}%'.format(els.epoch_counter, loss, val_acc * 100))\n",
    "    \n",
    "    tElapse = time() - tStart\n",
    "    saver.save(sess, './Team32_HW3_1.ckpt')\n",
    "    print('Model saved!')\n",
    "    \n",
    "    # Run test\n",
    "    y_pre = sess.run(y_prob, feed_dict={x: X_test2})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_test2, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: X_test2})\n",
    "    print('Test accuracy: {:.3f}%'.format(test_acc * 100))\n",
    "    print('Total training time: {:.3f}ms'.format(totalTime * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:   1, Loss: 1.79848647, Accuracy: 26.667%\n",
      "Epoch:   2, Loss: 1.65778708, Accuracy: 24.667%\n",
      "Epoch:   3, Loss: 1.51020586, Accuracy: 52.667%\n",
      "Epoch:   4, Loss: 1.43087649, Accuracy: 48.667%\n",
      "Epoch:   5, Loss: 1.36191952, Accuracy: 62.000%\n",
      "Epoch:   6, Loss: 1.30241823, Accuracy: 63.333%\n",
      "Epoch:   7, Loss: 1.25005722, Accuracy: 65.333%\n",
      "Epoch:   8, Loss: 1.20372045, Accuracy: 66.000%\n",
      "Epoch:   9, Loss: 1.16237962, Accuracy: 66.667%\n",
      "Epoch:  10, Loss: 1.12527347, Accuracy: 66.667%\n",
      "Epoch:  11, Loss: 1.09177649, Accuracy: 67.333%\n",
      "Epoch:  12, Loss: 1.06138396, Accuracy: 67.333%\n",
      "Epoch:  13, Loss: 1.03367972, Accuracy: 68.000%\n",
      "Epoch:  14, Loss: 1.00831890, Accuracy: 68.000%\n",
      "Epoch:  15, Loss: 0.98501265, Accuracy: 68.667%\n",
      "Epoch:  16, Loss: 0.96351731, Accuracy: 69.333%\n",
      "Epoch:  17, Loss: 0.94362587, Accuracy: 69.333%\n",
      "Epoch:  18, Loss: 0.92516178, Accuracy: 69.333%\n",
      "Epoch:  19, Loss: 0.90797287, Accuracy: 68.667%\n",
      "Epoch:  20, Loss: 0.89192790, Accuracy: 69.333%\n",
      "Epoch:  21, Loss: 0.87691265, Accuracy: 69.333%\n",
      "Epoch:  22, Loss: 0.86282772, Accuracy: 69.333%\n",
      "Epoch:  23, Loss: 0.84958583, Accuracy: 69.333%\n",
      "Epoch:  24, Loss: 0.83711034, Accuracy: 69.333%\n",
      "Epoch:  25, Loss: 0.82533365, Accuracy: 69.333%\n",
      "Epoch:  26, Loss: 0.81419545, Accuracy: 70.000%\n",
      "Epoch:  27, Loss: 0.80364251, Accuracy: 70.000%\n",
      "Epoch:  28, Loss: 0.79362726, Accuracy: 70.000%\n",
      "Epoch:  29, Loss: 0.78410697, Accuracy: 70.667%\n",
      "Epoch:  30, Loss: 0.77504349, Accuracy: 70.667%\n",
      "Epoch:  31, Loss: 0.76640242, Accuracy: 70.667%\n",
      "Epoch:  32, Loss: 0.75815272, Accuracy: 70.667%\n",
      "Epoch:  33, Loss: 0.75026637, Accuracy: 70.667%\n",
      "Epoch:  34, Loss: 0.74271792, Accuracy: 70.667%\n",
      "Epoch:  35, Loss: 0.73548424, Accuracy: 70.667%\n",
      "Epoch:  36, Loss: 0.72854441, Accuracy: 70.667%\n",
      "Epoch:  37, Loss: 0.72187918, Accuracy: 70.667%\n",
      "Epoch:  38, Loss: 0.71547097, Accuracy: 70.667%\n",
      "Epoch:  39, Loss: 0.70930374, Accuracy: 71.333%\n",
      "Epoch:  40, Loss: 0.70336270, Accuracy: 71.333%\n",
      "Epoch:  41, Loss: 0.69763440, Accuracy: 72.000%\n",
      "Epoch:  42, Loss: 0.69210631, Accuracy: 72.000%\n",
      "Epoch:  43, Loss: 0.68676704, Accuracy: 72.000%\n",
      "Epoch:  44, Loss: 0.68160576, Accuracy: 72.000%\n",
      "Epoch:  45, Loss: 0.67661279, Accuracy: 72.667%\n",
      "Epoch:  46, Loss: 0.67177892, Accuracy: 72.667%\n",
      "Epoch:  47, Loss: 0.66709584, Accuracy: 73.333%\n",
      "Epoch:  48, Loss: 0.66255563, Accuracy: 73.333%\n",
      "Epoch:  49, Loss: 0.65815103, Accuracy: 73.333%\n",
      "Epoch:  50, Loss: 0.65387517, Accuracy: 73.333%\n",
      "Epoch:  51, Loss: 0.64972168, Accuracy: 73.333%\n",
      "Epoch:  52, Loss: 0.64568472, Accuracy: 73.333%\n",
      "Epoch:  53, Loss: 0.64175880, Accuracy: 73.333%\n",
      "Epoch:  54, Loss: 0.63793862, Accuracy: 73.333%\n",
      "Epoch:  55, Loss: 0.63421935, Accuracy: 73.333%\n",
      "Epoch:  56, Loss: 0.63059646, Accuracy: 73.333%\n",
      "Epoch:  57, Loss: 0.62706578, Accuracy: 73.333%\n",
      "Epoch:  58, Loss: 0.62362313, Accuracy: 73.333%\n",
      "Epoch:  59, Loss: 0.62026489, Accuracy: 73.333%\n",
      "Epoch:  60, Loss: 0.61698735, Accuracy: 74.000%\n",
      "Epoch:  61, Loss: 0.61378723, Accuracy: 74.000%\n",
      "Epoch:  62, Loss: 0.61066133, Accuracy: 74.000%\n",
      "Epoch:  63, Loss: 0.60760677, Accuracy: 74.000%\n",
      "Epoch:  64, Loss: 0.60462070, Accuracy: 74.000%\n",
      "Epoch:  65, Loss: 0.60170031, Accuracy: 74.000%\n",
      "Epoch:  66, Loss: 0.59884316, Accuracy: 74.000%\n",
      "Epoch:  67, Loss: 0.59604698, Accuracy: 74.000%\n",
      "Epoch:  68, Loss: 0.59330934, Accuracy: 74.000%\n",
      "Epoch:  69, Loss: 0.59062815, Accuracy: 74.000%\n",
      "Epoch:  70, Loss: 0.58800137, Accuracy: 74.000%\n",
      "Epoch:  71, Loss: 0.58542717, Accuracy: 74.000%\n",
      "Epoch:  72, Loss: 0.58290344, Accuracy: 74.000%\n",
      "Epoch:  73, Loss: 0.58042866, Accuracy: 74.000%\n",
      "Epoch:  74, Loss: 0.57800102, Accuracy: 74.000%\n",
      "Epoch:  75, Loss: 0.57561904, Accuracy: 74.667%\n",
      "Epoch:  76, Loss: 0.57328123, Accuracy: 74.667%\n",
      "Epoch:  77, Loss: 0.57098603, Accuracy: 74.667%\n",
      "Epoch:  78, Loss: 0.56873208, Accuracy: 74.667%\n",
      "Epoch:  79, Loss: 0.56651801, Accuracy: 74.667%\n",
      "Epoch:  80, Loss: 0.56434274, Accuracy: 74.667%\n",
      "Epoch:  81, Loss: 0.56220502, Accuracy: 74.667%\n",
      "Epoch:  82, Loss: 0.56010360, Accuracy: 74.667%\n",
      "Epoch:  83, Loss: 0.55803734, Accuracy: 74.667%\n",
      "Epoch:  84, Loss: 0.55600530, Accuracy: 74.667%\n",
      "Epoch:  85, Loss: 0.55400652, Accuracy: 74.667%\n",
      "Epoch:  86, Loss: 0.55203986, Accuracy: 74.667%\n",
      "Epoch:  87, Loss: 0.55010456, Accuracy: 74.667%\n",
      "Epoch:  88, Loss: 0.54819959, Accuracy: 74.667%\n",
      "Epoch:  89, Loss: 0.54632419, Accuracy: 74.667%\n",
      "Epoch:  90, Loss: 0.54447752, Accuracy: 74.667%\n",
      "Epoch:  91, Loss: 0.54265875, Accuracy: 74.667%\n",
      "Epoch:  92, Loss: 0.54086721, Accuracy: 74.667%\n",
      "Epoch:  93, Loss: 0.53910196, Accuracy: 74.667%\n",
      "Epoch:  94, Loss: 0.53736258, Accuracy: 74.667%\n",
      "Epoch:  95, Loss: 0.53564823, Accuracy: 74.667%\n",
      "Model saved!\n",
      "Test accuracy: 77.227%\n",
      "Total training time: 210.316ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMuch faster than HW3.1(162.935ms vs 372.966ms)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5\n",
    "\n",
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n",
    "\n",
    "'''\n",
    "Homework 3-2\n",
    "'''\n",
    "\n",
    "#Early stop mechanism\n",
    "class early_stop():\n",
    "    def __init__(self, stop_margin, max_epochs):\n",
    "        self.val_not_better_counter = 0 # Count epoch after best validation error > stop_margin then stop\n",
    "        self.stop_margin = stop_margin # Count epochs if no progress\n",
    "        self.epoch_counter = 0 # Record epoch count\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.best_acc = 0 # Record best accuracy so far\n",
    "        \n",
    "    def monitor(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc # If accuracy better than before, record it as best result so far\n",
    "            self.val_not_better_counter = 0 # Reset counter\n",
    "        else:\n",
    "            self.val_not_better_counter += 1 # If no progress, counter++\n",
    "        self.epoch_counter += 1 # Count epochs\n",
    "            \n",
    "    def continue_training(self):\n",
    "        # Continue training if no progress epochs < stop_margin epochs and current epoch < max_epochs\n",
    "        return self.val_not_better_counter < self.stop_margin and self.epoch_counter < self.max_epochs\n",
    "    \n",
    "num_class = 5\n",
    "\n",
    "# Convert label to one-hot encoding matrix\n",
    "def label_to_onehot(label):\n",
    "    result = np.empty([label.shape[0], num_class])\n",
    "    for i in range(label.shape[0]):\n",
    "        enc = np.zeros(5)\n",
    "        enc[label[i]] = 1.0\n",
    "        result[i] = enc\n",
    "    return result\n",
    "\n",
    "y_train2 = label_to_onehot(y_train2)\n",
    "y_valid2 = label_to_onehot(y_valid2)\n",
    "y_test2 = label_to_onehot(y_test2)\n",
    "\n",
    "# Load model from homework2\n",
    "restore_saver = tf.train.import_meta_graph('./Team32_HW2.ckpt.meta')\n",
    "\n",
    "# Get necessary variables/tensors\n",
    "x = tf.get_default_graph().get_tensor_by_name('x:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "loss_func = tf.get_default_graph().get_tensor_by_name('loss:0')\n",
    "y_prob = tf.get_default_graph().get_tensor_by_name('y_prob:0')\n",
    "output_layer_vars_W = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Wo')\n",
    "output_layer_vars_b = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='bo')\n",
    "# Get 5th hidden layer's tensor for compute\n",
    "out5 = tf.get_default_graph().get_tensor_by_name('out5:0')\n",
    "# Train only on selected variables\n",
    "training_op = tf.train.GradientDescentOptimizer(1).minimize(loss_func, var_list=[output_layer_vars_W, output_layer_vars_b])\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #init\n",
    "    els = early_stop(stop_margin = 20,\n",
    "                    max_epochs = 1000)\n",
    "    \n",
    "    # Record total training time\n",
    "    totalTime = 0\n",
    "    # Get 5th hidden layer's output in order to cache\n",
    "    X_cache = sess.run(out5, feed_dict={x: X_train2})\n",
    "    while(els.continue_training()):\n",
    "        tStart = time()\n",
    "        # Feed cached set directly into softmax\n",
    "        _, loss = sess.run([training_op, loss_func], feed_dict={out5: X_cache, y: y_train2})\n",
    "        totalTime += time() - tStart\n",
    "        loss = np.mean(loss)\n",
    "        \n",
    "        #validate\n",
    "        y_pre = sess.run(y_prob, feed_dict={x: X_valid2})\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_valid2, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        val_acc = sess.run(accuracy, feed_dict={x: X_valid2})\n",
    "        els.monitor(val_acc)\n",
    "        print('Epoch: {:3}, Loss: {:.8f}, Accuracy: {:.3f}%'.format(els.epoch_counter, loss, val_acc * 100))\n",
    "    \n",
    "    tElapse = time() - tStart\n",
    "    saver.save(sess, './Team32_HW3_2.ckpt')\n",
    "    print('Model saved!')\n",
    "    \n",
    "    # Run test\n",
    "    y_pre = sess.run(y_prob, feed_dict={x: X_test2})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_test2, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: X_test2})\n",
    "    print('Test accuracy: {:.3f}%'.format(test_acc * 100))\n",
    "    print('Total training time: {:.3f}ms'.format(tElapse * 1000))\n",
    "    \n",
    "\"\"\"\n",
    "Much faster than HW3.1(210.316ms vs 686.030ms)\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:   1, Loss: 1.73934174, Accuracy: 39.333%\n",
      "Epoch:   2, Loss: 1.51412582, Accuracy: 52.000%\n",
      "Epoch:   3, Loss: 1.40322518, Accuracy: 60.000%\n",
      "Epoch:   4, Loss: 1.31406975, Accuracy: 67.333%\n",
      "Epoch:   5, Loss: 1.23960829, Accuracy: 68.000%\n",
      "Epoch:   6, Loss: 1.17652893, Accuracy: 68.000%\n",
      "Epoch:   7, Loss: 1.12244964, Accuracy: 68.667%\n",
      "Epoch:   8, Loss: 1.07559168, Accuracy: 69.333%\n",
      "Epoch:   9, Loss: 1.03460121, Accuracy: 69.333%\n",
      "Epoch:  10, Loss: 0.99843526, Accuracy: 70.000%\n",
      "Epoch:  11, Loss: 0.96627957, Accuracy: 72.000%\n",
      "Epoch:  12, Loss: 0.93749022, Accuracy: 72.667%\n",
      "Epoch:  13, Loss: 0.91155297, Accuracy: 73.333%\n",
      "Epoch:  14, Loss: 0.88805157, Accuracy: 74.667%\n",
      "Epoch:  15, Loss: 0.86664677, Accuracy: 74.667%\n",
      "Epoch:  16, Loss: 0.84705877, Accuracy: 74.667%\n",
      "Epoch:  17, Loss: 0.82905567, Accuracy: 74.667%\n",
      "Epoch:  18, Loss: 0.81244278, Accuracy: 76.000%\n",
      "Epoch:  19, Loss: 0.79705656, Accuracy: 76.000%\n",
      "Epoch:  20, Loss: 0.78275770, Accuracy: 76.000%\n",
      "Epoch:  21, Loss: 0.76942784, Accuracy: 76.000%\n",
      "Epoch:  22, Loss: 0.75696486, Accuracy: 76.000%\n",
      "Epoch:  23, Loss: 0.74528086, Accuracy: 76.000%\n",
      "Epoch:  24, Loss: 0.73429948, Accuracy: 76.000%\n",
      "Epoch:  25, Loss: 0.72395396, Accuracy: 76.000%\n",
      "Epoch:  26, Loss: 0.71418625, Accuracy: 76.000%\n",
      "Epoch:  27, Loss: 0.70494485, Accuracy: 76.667%\n",
      "Epoch:  28, Loss: 0.69618452, Accuracy: 76.667%\n",
      "Epoch:  29, Loss: 0.68786502, Accuracy: 76.000%\n",
      "Epoch:  30, Loss: 0.67995077, Accuracy: 76.000%\n",
      "Epoch:  31, Loss: 0.67240971, Accuracy: 76.000%\n",
      "Epoch:  32, Loss: 0.66521335, Accuracy: 76.000%\n",
      "Epoch:  33, Loss: 0.65833592, Accuracy: 75.333%\n",
      "Epoch:  34, Loss: 0.65175438, Accuracy: 75.333%\n",
      "Epoch:  35, Loss: 0.64544779, Accuracy: 75.333%\n",
      "Epoch:  36, Loss: 0.63939732, Accuracy: 75.333%\n",
      "Epoch:  37, Loss: 0.63358563, Accuracy: 75.333%\n",
      "Epoch:  38, Loss: 0.62799728, Accuracy: 75.333%\n",
      "Epoch:  39, Loss: 0.62261790, Accuracy: 75.333%\n",
      "Epoch:  40, Loss: 0.61743444, Accuracy: 75.333%\n",
      "Epoch:  41, Loss: 0.61243498, Accuracy: 75.333%\n",
      "Epoch:  42, Loss: 0.60760856, Accuracy: 75.333%\n",
      "Epoch:  43, Loss: 0.60294503, Accuracy: 75.333%\n",
      "Epoch:  44, Loss: 0.59843516, Accuracy: 75.333%\n",
      "Epoch:  45, Loss: 0.59407043, Accuracy: 75.333%\n",
      "Epoch:  46, Loss: 0.58984280, Accuracy: 75.333%\n",
      "Epoch:  47, Loss: 0.58574498, Accuracy: 76.000%\n",
      "Model saved!\n",
      "Test accuracy: 78.050%\n",
      "Total training time: 191.288ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBetter than HW3.2(79.757% vs 77.227%)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5\n",
    "\n",
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n",
    "\n",
    "'''\n",
    "Homework 3-3\n",
    "'''\n",
    "\n",
    "#Early stop mechanism\n",
    "class early_stop():\n",
    "    def __init__(self, stop_margin, max_epochs):\n",
    "        self.val_not_better_counter = 0 # Count epoch after best validation error > stop_margin then stop\n",
    "        self.stop_margin = stop_margin # Count epochs if no progress\n",
    "        self.epoch_counter = 0 # Record epoch count\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.best_acc = 0 # Record best accuracy so far\n",
    "        \n",
    "    def monitor(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc # If accuracy better than before, record it as best result so far\n",
    "            self.val_not_better_counter = 0 # Reset counter\n",
    "        else:\n",
    "            self.val_not_better_counter += 1 # If no progress, counter++\n",
    "        self.epoch_counter += 1 # Count epochs\n",
    "            \n",
    "    def continue_training(self):\n",
    "        # Continue training if no progress epochs < stop_margin epochs and current epoch < max_epochs\n",
    "        return self.val_not_better_counter < self.stop_margin and self.epoch_counter < self.max_epochs\n",
    "    \n",
    "num_class = 5\n",
    "hidden_neuron_size = 128\n",
    "\n",
    "# Convert label to one-hot encoding matrix\n",
    "def label_to_onehot(label):\n",
    "    result = np.empty([label.shape[0], num_class])\n",
    "    for i in range(label.shape[0]):\n",
    "        enc = np.zeros(5)\n",
    "        enc[label[i]] = 1.0\n",
    "        result[i] = enc\n",
    "    return result\n",
    "\n",
    "y_train2 = label_to_onehot(y_train2)\n",
    "y_valid2 = label_to_onehot(y_valid2)\n",
    "y_test2 = label_to_onehot(y_test2)\n",
    "\n",
    "# Load model from homework2\n",
    "restore_saver = tf.train.import_meta_graph('./Team32_HW2.ckpt.meta')\n",
    "\n",
    "# Get necessary variables/tensors\n",
    "x = tf.get_default_graph().get_tensor_by_name('x:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "y_prob = tf.get_default_graph().get_tensor_by_name('y_prob:0')\n",
    "# Get 4th hidden layer's tensor\n",
    "out4 = tf.get_default_graph().get_tensor_by_name('out4:0')\n",
    "\n",
    "def init_var(name, shape):\n",
    "    return tf.get_variable(name, shape, dtype=tf.float32, \n",
    "                           initializer=variance_scaling_initializer(factor=1.0))\n",
    "# Build a new softmax layer connected by 4th hidden layer\n",
    "Wn = init_var(\"Wn\", [hidden_neuron_size, num_class])\n",
    "bn = init_var(\"bn\", [num_class])\n",
    "logits = tf.add(tf.matmul(out4, Wn), bn)\n",
    "y_prob = tf.nn.softmax(logits)\n",
    "loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = logits))\n",
    "\n",
    "# Train only on selected variables\n",
    "# Only train new softmax layer\n",
    "training_op = tf.train.GradientDescentOptimizer(1).minimize(loss_func, var_list=[Wn, bn])\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #init\n",
    "    els = early_stop(stop_margin = 20,\n",
    "                    max_epochs = 1000)\n",
    "    \n",
    "    # Record total training time\n",
    "    totalTime = 0        \n",
    "    while(els.continue_training()):\n",
    "        tStart = time()\n",
    "        _, loss = sess.run([training_op, loss_func], feed_dict={x: X_train2, y: y_train2})\n",
    "        totalTime += time() - tStart\n",
    "        loss = np.mean(loss)\n",
    "        \n",
    "        #validate\n",
    "        y_pre = sess.run(y_prob, feed_dict={x: X_valid2})\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_valid2, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        val_acc = sess.run(accuracy, feed_dict={x: X_valid2})\n",
    "        \n",
    "        els.monitor(val_acc)\n",
    "        print('Epoch: {:3}, Loss: {:.8f}, Accuracy: {:.3f}%'.format(els.epoch_counter, loss, val_acc * 100))\n",
    "    \n",
    "    tElapse = time() - tStart\n",
    "    saver.save(sess, './Team32_HW3_3.ckpt')\n",
    "    print('Model saved!')\n",
    "    \n",
    "    # Run test\n",
    "    y_pre = sess.run(y_prob, feed_dict={x: X_test2})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_test2, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: X_test2})\n",
    "    print('Test accuracy: {:.3f}%'.format(test_acc * 100))\n",
    "    print('Total training time: {:.3f}ms'.format(tElapse * 1000))\n",
    "    \n",
    "\"\"\"\n",
    "Better than HW3.2(78.050% vs 77.227%)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:   1, Loss: 1.73934174, Accuracy: 16.000%\n",
      "Epoch:   2, Loss: 1.61377764, Accuracy: 27.333%\n",
      "Epoch:   3, Loss: 1.51546311, Accuracy: 48.667%\n",
      "Epoch:   4, Loss: 1.43313181, Accuracy: 64.667%\n",
      "Epoch:   5, Loss: 1.35915375, Accuracy: 70.667%\n",
      "Epoch:   6, Loss: 1.28832793, Accuracy: 74.667%\n",
      "Epoch:   7, Loss: 1.21826220, Accuracy: 76.667%\n",
      "Epoch:   8, Loss: 1.14852190, Accuracy: 77.333%\n",
      "Epoch:   9, Loss: 1.07970583, Accuracy: 78.000%\n",
      "Epoch:  10, Loss: 1.01282287, Accuracy: 78.000%\n",
      "Epoch:  11, Loss: 0.94894946, Accuracy: 80.667%\n",
      "Epoch:  12, Loss: 0.88893229, Accuracy: 82.000%\n",
      "Epoch:  13, Loss: 0.83328027, Accuracy: 82.000%\n",
      "Epoch:  14, Loss: 0.78219533, Accuracy: 82.667%\n",
      "Epoch:  15, Loss: 0.73561198, Accuracy: 82.667%\n",
      "Epoch:  16, Loss: 0.69328332, Accuracy: 83.333%\n",
      "Epoch:  17, Loss: 0.65487736, Accuracy: 83.333%\n",
      "Epoch:  18, Loss: 0.62001741, Accuracy: 83.333%\n",
      "Epoch:  19, Loss: 0.58832681, Accuracy: 84.667%\n",
      "Epoch:  20, Loss: 0.55945218, Accuracy: 84.667%\n",
      "Epoch:  21, Loss: 0.53307676, Accuracy: 85.333%\n",
      "Epoch:  22, Loss: 0.50891846, Accuracy: 85.333%\n",
      "Epoch:  23, Loss: 0.48673043, Accuracy: 85.333%\n",
      "Epoch:  24, Loss: 0.46629634, Accuracy: 85.333%\n",
      "Epoch:  25, Loss: 0.44742939, Accuracy: 85.333%\n",
      "Epoch:  26, Loss: 0.42996657, Accuracy: 86.000%\n",
      "Epoch:  27, Loss: 0.41376397, Accuracy: 86.000%\n",
      "Epoch:  28, Loss: 0.39869457, Accuracy: 86.000%\n",
      "Epoch:  29, Loss: 0.38464758, Accuracy: 86.000%\n",
      "Epoch:  30, Loss: 0.37152487, Accuracy: 86.000%\n",
      "Epoch:  31, Loss: 0.35923934, Accuracy: 86.000%\n",
      "Epoch:  32, Loss: 0.34771380, Accuracy: 86.000%\n",
      "Epoch:  33, Loss: 0.33687952, Accuracy: 86.000%\n",
      "Epoch:  34, Loss: 0.32667547, Accuracy: 86.000%\n",
      "Epoch:  35, Loss: 0.31704703, Accuracy: 86.000%\n",
      "Epoch:  36, Loss: 0.30794549, Accuracy: 86.667%\n",
      "Epoch:  37, Loss: 0.29932678, Accuracy: 86.667%\n",
      "Epoch:  38, Loss: 0.29115123, Accuracy: 86.667%\n",
      "Epoch:  39, Loss: 0.28338340, Accuracy: 86.667%\n",
      "Epoch:  40, Loss: 0.27599186, Accuracy: 86.667%\n",
      "Epoch:  41, Loss: 0.26894802, Accuracy: 86.667%\n",
      "Epoch:  42, Loss: 0.26222593, Accuracy: 86.667%\n",
      "Epoch:  43, Loss: 0.25580207, Accuracy: 87.333%\n",
      "Epoch:  44, Loss: 0.24965483, Accuracy: 87.333%\n",
      "Epoch:  45, Loss: 0.24376477, Accuracy: 87.333%\n",
      "Epoch:  46, Loss: 0.23811421, Accuracy: 87.333%\n",
      "Epoch:  47, Loss: 0.23268713, Accuracy: 87.333%\n",
      "Epoch:  48, Loss: 0.22746877, Accuracy: 87.333%\n",
      "Epoch:  49, Loss: 0.22244586, Accuracy: 88.000%\n",
      "Epoch:  50, Loss: 0.21760614, Accuracy: 88.000%\n",
      "Epoch:  51, Loss: 0.21293820, Accuracy: 88.000%\n",
      "Epoch:  52, Loss: 0.20843174, Accuracy: 88.000%\n",
      "Epoch:  53, Loss: 0.20407709, Accuracy: 88.000%\n",
      "Epoch:  54, Loss: 0.19986552, Accuracy: 88.000%\n",
      "Epoch:  55, Loss: 0.19578896, Accuracy: 88.000%\n",
      "Epoch:  56, Loss: 0.19184014, Accuracy: 88.000%\n",
      "Epoch:  57, Loss: 0.18801224, Accuracy: 88.000%\n",
      "Epoch:  58, Loss: 0.18429899, Accuracy: 88.000%\n",
      "Epoch:  59, Loss: 0.18069474, Accuracy: 88.000%\n",
      "Epoch:  60, Loss: 0.17719445, Accuracy: 88.000%\n",
      "Epoch:  61, Loss: 0.17379321, Accuracy: 88.000%\n",
      "Epoch:  62, Loss: 0.17048660, Accuracy: 88.000%\n",
      "Epoch:  63, Loss: 0.16727030, Accuracy: 88.000%\n",
      "Epoch:  64, Loss: 0.16414046, Accuracy: 89.333%\n",
      "Epoch:  65, Loss: 0.16109338, Accuracy: 89.333%\n",
      "Epoch:  66, Loss: 0.15812561, Accuracy: 89.333%\n",
      "Epoch:  67, Loss: 0.15523389, Accuracy: 89.333%\n",
      "Epoch:  68, Loss: 0.15241522, Accuracy: 90.000%\n",
      "Epoch:  69, Loss: 0.14966679, Accuracy: 90.000%\n",
      "Epoch:  70, Loss: 0.14698601, Accuracy: 90.000%\n",
      "Epoch:  71, Loss: 0.14437041, Accuracy: 90.667%\n",
      "Epoch:  72, Loss: 0.14181761, Accuracy: 90.667%\n",
      "Epoch:  73, Loss: 0.13932551, Accuracy: 90.667%\n",
      "Epoch:  74, Loss: 0.13689210, Accuracy: 90.667%\n",
      "Epoch:  75, Loss: 0.13451554, Accuracy: 90.667%\n",
      "Epoch:  76, Loss: 0.13219400, Accuracy: 90.667%\n",
      "Epoch:  77, Loss: 0.12992585, Accuracy: 91.333%\n",
      "Epoch:  78, Loss: 0.12770954, Accuracy: 91.333%\n",
      "Epoch:  79, Loss: 0.12554358, Accuracy: 91.333%\n",
      "Epoch:  80, Loss: 0.12342653, Accuracy: 91.333%\n",
      "Epoch:  81, Loss: 0.12135717, Accuracy: 92.000%\n",
      "Epoch:  82, Loss: 0.11933415, Accuracy: 92.000%\n",
      "Epoch:  83, Loss: 0.11735620, Accuracy: 92.000%\n",
      "Epoch:  84, Loss: 0.11542204, Accuracy: 92.000%\n",
      "Epoch:  85, Loss: 0.11353055, Accuracy: 92.000%\n",
      "Epoch:  86, Loss: 0.11168054, Accuracy: 92.000%\n",
      "Epoch:  87, Loss: 0.10987098, Accuracy: 92.000%\n",
      "Epoch:  88, Loss: 0.10810083, Accuracy: 92.000%\n",
      "Epoch:  89, Loss: 0.10636930, Accuracy: 92.000%\n",
      "Epoch:  90, Loss: 0.10467535, Accuracy: 92.000%\n",
      "Epoch:  91, Loss: 0.10301800, Accuracy: 92.000%\n",
      "Epoch:  92, Loss: 0.10139628, Accuracy: 92.000%\n",
      "Epoch:  93, Loss: 0.09980930, Accuracy: 92.000%\n",
      "Epoch:  94, Loss: 0.09825613, Accuracy: 92.000%\n",
      "Epoch:  95, Loss: 0.09673589, Accuracy: 92.667%\n",
      "Epoch:  96, Loss: 0.09524776, Accuracy: 92.667%\n",
      "Epoch:  97, Loss: 0.09379082, Accuracy: 92.667%\n",
      "Epoch:  98, Loss: 0.09236445, Accuracy: 92.667%\n",
      "Epoch:  99, Loss: 0.09096772, Accuracy: 92.667%\n",
      "Epoch: 100, Loss: 0.08959994, Accuracy: 92.667%\n",
      "Epoch: 101, Loss: 0.08826036, Accuracy: 92.667%\n",
      "Epoch: 102, Loss: 0.08694822, Accuracy: 92.667%\n",
      "Epoch: 103, Loss: 0.08566283, Accuracy: 92.667%\n",
      "Epoch: 104, Loss: 0.08440348, Accuracy: 92.667%\n",
      "Epoch: 105, Loss: 0.08316951, Accuracy: 92.667%\n",
      "Epoch: 106, Loss: 0.08196028, Accuracy: 92.667%\n",
      "Epoch: 107, Loss: 0.08077518, Accuracy: 92.667%\n",
      "Epoch: 108, Loss: 0.07961357, Accuracy: 92.667%\n",
      "Epoch: 109, Loss: 0.07847487, Accuracy: 92.667%\n",
      "Epoch: 110, Loss: 0.07735853, Accuracy: 92.667%\n",
      "Epoch: 111, Loss: 0.07626396, Accuracy: 92.667%\n",
      "Epoch: 112, Loss: 0.07519066, Accuracy: 92.667%\n",
      "Epoch: 113, Loss: 0.07413808, Accuracy: 92.667%\n",
      "Epoch: 114, Loss: 0.07310572, Accuracy: 92.667%\n",
      "Epoch: 115, Loss: 0.07209311, Accuracy: 92.667%\n",
      "Model saved!\n",
      "Test accuracy: 89.961%\n",
      "Total training time: 231.348ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMuch better than HW3.2(89.961% vs 79.757%)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# load data: digits 5 to 9, but still label with 0 to 4, \n",
    "# because TensorFlow expects label's integers from 0 to n_classes-1.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train2_full = mnist.train.images[mnist.train.labels >= 5]\n",
    "y_train2_full = mnist.train.labels[mnist.train.labels >= 5] - 5\n",
    "X_valid2_full = mnist.validation.images[mnist.validation.labels >= 5]\n",
    "y_valid2_full = mnist.validation.labels[mnist.validation.labels >= 5] - 5\n",
    "X_test2 = mnist.test.images[mnist.test.labels >= 5]\n",
    "y_test2 = mnist.test.labels[mnist.test.labels >= 5] - 5\n",
    "\n",
    "# we want to keep only 100 instances per class in the training set \n",
    "# and let's keep only 30 instances per class in the validation set\n",
    "# tesing set is already loaded above\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n",
    "\n",
    "'''\n",
    "Homework 3-4\n",
    "'''\n",
    "\n",
    "#Early stop mechanism\n",
    "class early_stop():\n",
    "    def __init__(self, stop_margin, max_epochs):\n",
    "        self.val_not_better_counter = 0 # Count epoch after best validation error > stop_margin then stop\n",
    "        self.stop_margin = stop_margin # Count epochs if no progress\n",
    "        self.epoch_counter = 0 # Record epoch count\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.best_acc = 0 # Record best accuracy so far\n",
    "        \n",
    "    def monitor(self, val_acc):\n",
    "        if val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc # If accuracy better than before, record it as best result so far\n",
    "            self.val_not_better_counter = 0 # Reset counter\n",
    "        else:\n",
    "            self.val_not_better_counter += 1 # If no progress, counter++\n",
    "        self.epoch_counter += 1 # Count epochs\n",
    "            \n",
    "    def continue_training(self):\n",
    "        # Continue training if no progress epochs < stop_margin epochs and current epoch < max_epochs\n",
    "        return self.val_not_better_counter < self.stop_margin and self.epoch_counter < self.max_epochs\n",
    "    \n",
    "num_class = 5\n",
    "hidden_neuron_size = 128\n",
    "\n",
    "# Convert label to one-hot encoding matrix\n",
    "def label_to_onehot(label):\n",
    "    result = np.empty([label.shape[0], num_class])\n",
    "    for i in range(label.shape[0]):\n",
    "        enc = np.zeros(5)\n",
    "        enc[label[i]] = 1.0\n",
    "        result[i] = enc\n",
    "    return result\n",
    "\n",
    "y_train2 = label_to_onehot(y_train2)\n",
    "y_valid2 = label_to_onehot(y_valid2)\n",
    "y_test2 = label_to_onehot(y_test2)\n",
    "\n",
    "# Load model from homework2\n",
    "restore_saver = tf.train.import_meta_graph('./Team32_HW2.ckpt.meta')\n",
    "\n",
    "# Get necessary variables/tensors\n",
    "x = tf.get_default_graph().get_tensor_by_name('x:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "y_prob = tf.get_default_graph().get_tensor_by_name('y_prob:0')\n",
    "# Get variables of first and second layers to train\n",
    "W1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='W1')\n",
    "b1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='b1')\n",
    "W2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='W2')\n",
    "b2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='b2')\n",
    "out4 = tf.get_default_graph().get_tensor_by_name('out4:0')\n",
    "\n",
    "def init_var(name, shape):\n",
    "    return tf.get_variable(name, shape, dtype=tf.float32, \n",
    "                           initializer=variance_scaling_initializer(factor=1.0))\n",
    "# Let new weight and bias variables for new softmax layer\n",
    "Wn = init_var(\"Wn\", [hidden_neuron_size, num_class])\n",
    "bn = init_var(\"bn\", [num_class])\n",
    "logits = tf.add(tf.matmul(out4, Wn), bn)\n",
    "y_prob = tf.nn.softmax(logits)\n",
    "loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = logits))\n",
    "\n",
    "# Train only on selected variables\n",
    "# Only train first, second hidden layers, and new softmax layer\n",
    "training_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss_func, var_list=[W1, b1, W2, b2, Wn, bn])\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #init\n",
    "    els = early_stop(stop_margin = 20,\n",
    "                    max_epochs = 1000)\n",
    "    \n",
    "    # Record total training time\n",
    "    totalTime = 0        \n",
    "    while(els.continue_training()):\n",
    "        tStart = time()\n",
    "        _, loss = sess.run([training_op, loss_func], feed_dict={x: X_train2, y: y_train2})\n",
    "        totalTime += time() - tStart\n",
    "        loss = np.mean(loss)\n",
    "        \n",
    "        #validate\n",
    "        y_pre = sess.run(y_prob, feed_dict={x: X_valid2})\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_valid2, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        val_acc = sess.run(accuracy, feed_dict={x: X_valid2})\n",
    "        \n",
    "        els.monitor(val_acc)\n",
    "        print('Epoch: {:3}, Loss: {:.8f}, Accuracy: {:.3f}%'.format(els.epoch_counter, loss, val_acc * 100))\n",
    "    \n",
    "    tElapse = time() - tStart\n",
    "    saver.save(sess, './Team32_HW3_4.ckpt')\n",
    "    print('Model saved!')\n",
    "    \n",
    "    # Run test\n",
    "    y_pre = sess.run(y_prob, feed_dict={x: X_test2})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(y_test2, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: X_test2})\n",
    "    print('Test accuracy: {:.3f}%'.format(test_acc * 100))\n",
    "    print('Total training time: {:.3f}ms'.format(tElapse * 1000))\n",
    "    \n",
    "\"\"\"\n",
    "Much better than HW3.2(89.961% vs 78.050%)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
